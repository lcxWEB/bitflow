{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10740e78-4234-4f3b-bccb-abfd87f0f48d",
   "metadata": {},
   "source": [
    "# 4.Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9d9440-52d5-4355-a280-6dedd9397a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-24 22:52:09.651148: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "sys.path.append('../config')  \n",
    "from model_config import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7411918f-6df3-4962-9add-90a6b46cbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scalers\n",
    "scaler_X = joblib.load('../models/scaler_X.pkl')\n",
    "scaler_y = joblib.load('../models/scaler_y.pkl')\n",
    "\n",
    "# Load normalized data\n",
    "normalized_data = joblib.load('../models/normalized_data.pkl')\n",
    "X_normalized = normalized_data['X_normalized']\n",
    "y_normalized = normalized_data['y_normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0414c7-1628-4fd9-ba2f-0fc83fc81a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create sequences for time series prediction\n",
    "def create_sequences(X, y, time_steps= ModelConfig.TIME_STEPS):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction\n",
    "    Args:\n",
    "        X: Input features DataFrame\n",
    "        y: Target values\n",
    "        time_steps: Number of time steps to look back\n",
    "    Returns:\n",
    "        Arrays of sequences for X and y\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc8539d-d830-4347-bf66-1a0c4a5c46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input sequences: (4630, 30, 4)\n",
      "Shape of the output sequences: (4630, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences\n",
    "X_seq, y_seq = create_sequences(X_normalized, y_normalized, ModelConfig.TIME_STEPS)\n",
    "print(\"Shape of the input sequences:\", X_seq.shape)\n",
    "print(\"Shape of the output sequences:\", y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bdafd8-40a9-4f0d-86b4-be73a7618513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_size = int(len(X_seq) * ModelConfig.TRAIN_SPLIT)\n",
    "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_test = y_seq[:train_size], y_seq[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3a1c24-7c65-47ca-b2ab-5ba8083909da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (3704, 30, 4)\n",
      "Testing Data Shape: (926, 30, 4)\n",
      "Training Labels Shape: (3704, 1)\n",
      "Testing Labels Shape: (926, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)\n",
    "print(\"Training Labels Shape:\", y_train.shape)\n",
    "print(\"Testing Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494a08d-480e-4fbe-ae70-4b1d8e00e4a4",
   "metadata": {},
   "source": [
    "# 4.1 LSTM Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a11e5d6-54b8-4bee-b8b8-966520ae7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "#Create LSTM model\n",
    "def create_lstm_model(input_shape, units=[64, 32], dropout=0.5):\n",
    "    \"\"\"\n",
    "    Create LSTM model with original configuration\n",
    "    Args:\n",
    "        input_shape: Shape of input data (TIME_STEPS, features)\n",
    "        units: List of units for LSTM layers [default: [64, 32]]\n",
    "        dropout: Dropout rate [default: 0.5]\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with return sequences\n",
    "    model.add(LSTM(\n",
    "        units[0],\n",
    "        return_sequences=True,\n",
    "        activation=ModelConfig.MODEL_PARAMS['activation'],\n",
    "        recurrent_activation=ModelConfig.MODEL_PARAMS['recurrent_activation'],\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units[1]))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b50bc1-8067-4c0c-9e15-4b7187029f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mxq/.pyenv/versions/my-env/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_lstm_model(\n",
    "    input_shape=(ModelConfig.TIME_STEPS, X_train.shape[2]),\n",
    "    units=ModelConfig.MODEL_PARAMS['units'],\n",
    "    dropout=ModelConfig.MODEL_PARAMS['dropout']\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=ModelConfig.TRAINING_PARAMS['learning_rate']),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7bbaf7-7618-4eaf-8c9f-a72f71c02286",
   "metadata": {},
   "source": [
    "## 4.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f84134a-198c-4bbd-9c2c-9ba032edd90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a dictionary to store timing metrics\n",
    "timing_metrics = {\n",
    "    'training_time': 0,\n",
    "    'prediction_time': 0,\n",
    "    'total_time': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "349c29d7-833c-4b4b-b88f-dd7b34ac06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - loss: 0.0046 - mae: 0.0387 - val_loss: 0.0069 - val_mae: 0.0610\n",
      "Epoch 2/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 1.8893e-04 - mae: 0.0093 - val_loss: 0.0119 - val_mae: 0.0835\n",
      "Epoch 3/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 1.5181e-04 - mae: 0.0082 - val_loss: 0.0075 - val_mae: 0.0642\n",
      "Epoch 4/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - loss: 1.2847e-04 - mae: 0.0076 - val_loss: 0.0024 - val_mae: 0.0348\n",
      "Epoch 5/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 1.5902e-04 - mae: 0.0086 - val_loss: 0.0080 - val_mae: 0.0653\n",
      "Epoch 6/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 9.9932e-05 - mae: 0.0070 - val_loss: 0.0026 - val_mae: 0.0374\n",
      "Epoch 7/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 1.4153e-04 - mae: 0.0082 - val_loss: 0.0075 - val_mae: 0.0639\n",
      "Epoch 8/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 89ms/step - loss: 1.0089e-04 - mae: 0.0073 - val_loss: 0.0094 - val_mae: 0.0717\n",
      "Epoch 9/100\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - loss: 1.1383e-04 - mae: 0.0076 - val_loss: 0.0105 - val_mae: 0.0800\n"
     ]
    }
   ],
   "source": [
    "# Training with timing\n",
    "print(\"Starting model training...\")\n",
    "training_start = time.time()\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',          # Monitor validation loss for improvement\n",
    "    patience=5,                  # Number of epochs to wait before stopping\n",
    "    restore_best_weights=True,   # Restore model weights from the epoch with the best value\n",
    "    min_delta=0.0001            # Minimum change to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Train model with early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=ModelConfig.TRAINING_PARAMS['epochs'],\n",
    "    batch_size=ModelConfig.TRAINING_PARAMS['batch_size'],\n",
    "    validation_split=ModelConfig.TRAINING_PARAMS['validation_split'],\n",
    "    callbacks=[early_stopping],  # Add early stopping callback\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e556d039-2581-40fd-aa5b-477cf0de7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_end = time.time()\n",
    "timing_metrics['training_time'] = training_end - training_start\n",
    "\n",
    "#Update model metrics dictionary to include early stopping information\n",
    "model_metrics = {\n",
    "    # Runtime metrics\n",
    "    'training_time': training_end - training_start,\n",
    "    \n",
    "    # Training history\n",
    "    'history': history.history,  # including loss, mae\n",
    "    \n",
    "    # Final metrics\n",
    "    'final_metrics': {\n",
    "        'mae': history.history['mae'][-1],          # Mean Absolute Error\n",
    "        'val_mae': history.history['val_mae'][-1],  # Validation MAE\n",
    "        'mape': history.history['mape'][-1] if 'mape' in history.history else None,  # MAPE if available\n",
    "        'stopped_epoch': early_stopping.stopped_epoch + 1 if early_stopping.stopped_epoch > 0 else None  # Add actual training epochs\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729c7802-ac1e-4910-9d03-7d39fbfa7fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed in 162.87 seconds\n",
      "Early stopping occurred at epoch 9\n"
     ]
    }
   ],
   "source": [
    "# save model and trainning time\n",
    "model.save('../models/lstm_model.keras')\n",
    "joblib.dump(history.history, '../models/training_history.pkl')\n",
    "joblib.dump(timing_metrics, '../models/training_metrics.pkl')\n",
    "\n",
    "# Print training summary\n",
    "print(f\"\\nTraining completed in {timing_metrics['training_time']:.2f} seconds\")\n",
    "if model_metrics['final_metrics']['stopped_epoch']:\n",
    "    print(f\"Early stopping occurred at epoch {model_metrics['final_metrics']['stopped_epoch']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
