{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "**Logistic Regression** is a supervised learning algorithm used for classification tasks. Despite the term \"regression\" in its name, logistic regression is widely used for binary classification problems rather than regression. It estimates the probability that a given input belongs to a certain category.\n",
    "\n",
    "## Analyze Salaries On Orange\n",
    "![](./logistic_regression_orange.png) \n",
    "\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. **Binary Classification**\n",
    "Logistic regression is primarily used for binary classification problems where the target variable has two possible outcomes, such as:\n",
    "- Spam vs. Not Spam\n",
    "- Purchased vs. Not Purchased\n",
    "- Disease vs. No Disease\n",
    "\n",
    "### 2. **Sigmoid Function**\n",
    "At the core of logistic regression is the **sigmoid function**, which maps any real-valued number to a value between 0 and 1. The function can be expressed as:\n",
    "\n",
    "$$ \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "\n",
    "Where $ z $ is the linear combination of the input features. The sigmoid function converts the output of a linear regression model into a probability score between 0 and 1.\n",
    "\n",
    "### 3. **Decision Boundary**\n",
    "The decision boundary is the threshold used to classify the output. Typically, the threshold is set at 0.5. If the sigmoid function's output is greater than 0.5, the data point is classified as class 1 (positive class), otherwise, it's classified as class 0 (negative class).\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases} \n",
    "1, & \\text{if } P(y=1|X) \\geq 0.5 \\\\\n",
    "0, & \\text{if } P(y=1|X) < 0.5 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 4. **Cost Function**\n",
    "Logistic regression uses a cost function based on maximum likelihood estimation, also known as the **log loss** or **binary cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y^{(i)}$ is the actual label (0 or 1),\n",
    "- $h_\\theta(x^{(i)})$ is the predicted probability,\n",
    "- $m$ is the number of training examples.\n",
    "\n",
    "### 5. **Gradient Descent**\n",
    "To minimize the cost function, logistic regression uses **gradient descent** to update the model's parameters (weights and bias). It calculates the gradient of the cost function and iteratively adjusts the parameters to find the minimum cost.\n",
    "\n",
    "### 6. **Multiclass Classification**\n",
    "Although logistic regression is commonly used for binary classification, it can be extended to multiclass classification using techniques like **One-vs-Rest (OvR)** or **Softmax Regression**.\n",
    "\n",
    "## Regularization in Logistic Regression\n",
    "\n",
    "### Why Regularization?\n",
    "In logistic regression, **overfitting** can occur when the model becomes too complex and captures noise in the training data rather than the underlying pattern. To prevent this, we apply regularization techniques like **Lasso (L1 regularization)** and **Ridge (L2 regularization)**, which add a penalty term to the cost function.\n",
    "\n",
    "### 1. **Lasso Regression (L1 Regularization)**\n",
    "\n",
    "- **Lasso** stands for **Least Absolute Shrinkage and Selection Operator**. In Lasso, the regularization term is the sum of the absolute values of the coefficients.\n",
    "  \n",
    "  The cost function for Lasso Logistic Regression is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{p} |\\theta_j|\n",
    "$$\n",
    "\n",
    "\n",
    "  Where $ \\lambda $ is the regularization parameter controlling the strength of regularization. Lasso tends to **shrink some coefficients to exactly zero**, which makes it useful for **feature selection**.\n",
    "\n",
    "\n",
    "#### Analysis Using Lasso\n",
    "\n",
    "![](./lasso_score.png) \n",
    "\n",
    "![](./lasso_confusion_matrix.png) \n",
    "\n",
    "### 2. **Ridge Regression (L2 Regularization)**\n",
    "\n",
    "- **Ridge** regression adds a penalty term based on the sum of the squared coefficients. This is known as **L2 regularization**. The cost function is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{p} \\theta_j^2\n",
    "$$\n",
    "\n",
    "  Unlike Lasso, Ridge regression **shrinks the coefficients** but does not force them to be zero. This means it retains all features but reduces the impact of less important ones.\n",
    "\n",
    "#### Analysis Using Ridge\n",
    "![](./ridge_score.png) \n",
    "\n",
    "![](./ridge_confusion_matrix.png) \n",
    "\n",
    "## Advantages of Regularization\n",
    "- Helps prevent overfitting by penalizing large coefficients.\n",
    "- Lasso can perform feature selection by shrinking irrelevant features to zero.\n",
    "- Ridge can handle multicollinearity, where input features are highly correlated.\n",
    "\n",
    "## Applications\n",
    "- Logistic regression with Lasso or Ridge regularization is used in:\n",
    "  - Medical diagnosis (e.g., predicting disease presence)\n",
    "  - Spam detection\n",
    "  - Credit scoring and fraud detection\n",
    "  - Predicting customer churn\n",
    "\n",
    "## Conclusion\n",
    "Logistic regression is a fundamental algorithm for classification problems, and with the addition of regularization techniques like Lasso and Ridge, it becomes even more powerful in handling overfitting and selecting important features. These methods ensure that the model generalizes well to unseen data, making them valuable tools in data science and machine learning.\n",
    "From the analysis of data scientist salaries, the Lasso and Ridge has similar performance. Lasso is slightly better than Ridge.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-explore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
